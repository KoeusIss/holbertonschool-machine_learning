{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38664bitkagglenvvenvc34dc818dda7470d8072550ee32c2b15",
   "display_name": "Python 3.5.2  ('mlenv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ef6f7fb1661f7c7bc9642098ef3b79f0ca28fe7e5403c5f390f3fd65f776b30e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import GPy\n",
    "import GPyOpt\n",
    "\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "\n",
    "# Importing dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Getting featurees and labels\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten data\n",
    "m, h, w = X_train.shape\n",
    "mv, _, _ = X_test.shape\n",
    "X_train = X_train.reshape(m, h * w)\n",
    "X_test = X_test.reshape(mv, h * w)\n",
    "\n",
    "# Data Normalization\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# Labels Encoding\n",
    "Y_train_oh = K.utils.to_categorical(Y_train)\n",
    "Y_test_oh = K.utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr, beta1, beta2, l2_rate):\n",
    "    \"\"\"Creates a model\n",
    "    \"\"\"\n",
    "    regularizer = K.regularizers.l2(l2_rate)\n",
    "    inputs = K.layers.Input(shape=(784,))\n",
    "\n",
    "    first = K.layers.Dense(\n",
    "        150,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer,\n",
    "        name=\"first\"\n",
    "    )(inputs)\n",
    "    second = K.layers.Dense(\n",
    "        60,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer,\n",
    "        name=\"second\"\n",
    "    )(first)\n",
    "    third = K.layers.Dense(\n",
    "        120,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer,\n",
    "        name=\"third\"\n",
    "    )(second)\n",
    "    outputs = K.layers.Dense(\n",
    "        10,\n",
    "        activation='softmax',\n",
    "        name=\"outputs\"\n",
    "    )(third)\n",
    "    model = K.Model(inputs, outputs)\n",
    "\n",
    "    adam = K.optimizers.Adam(\n",
    "        lr=lr,\n",
    "        beta_1=beta1,\n",
    "        beta_2=beta2\n",
    "    )\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=adam,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(params):\n",
    "    \"\"\"Fit a model\"\"\"\n",
    "    model = create_model(params[0][0], 0.9, 0.9, 0.1, 1e-4)\n",
    "\n",
    "    blackbox = model.fit(\n",
    "        X_train,\n",
    "        Y_train_oh,\n",
    "        batch_size=100,\n",
    "        epochs=5,\n",
    "        validation_split=0.15,\n",
    "        verbose=0\n",
    "    )\n",
    "    # accuracy = blackbox.history['val_acc'][-1]\n",
    "    val_accuracy = np.min(blackbox.history[\"val_loss\"])\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = [{\"name\": \"lr\", \"type\": \"continuous\", \"domain\": (0.01, 0.001)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bayesian_opt = BayesianOptimization(\n",
    "    fit_model,\n",
    "    domain=domain,\n",
    "    model_type=\"GP\",\n",
    "    initial_design_numdata=1,\n",
    "    acquisition_type=\"EI\",\n",
    "    maximize=False,\n",
    "    verbosity=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bayesian_opt.run_optimization(max_iter=29,\n",
    "                                         report_file=\"report\",\n",
    "                                         evaluations_file=\"evaluation\",\n",
    "                                         models_file=\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0fc74fdc2126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_bayesian_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_acquisition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ml/holbertonschool-machine_learning/mlenv/lib/python3.5/site-packages/GPyOpt/core/bo.py\u001b[0m in \u001b[0;36mplot_acquisition\u001b[0;34m(self, filename, label_x, label_y)\u001b[0m\n\u001b[1;32m    291\u001b[0m                                 \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                                 \u001b[0mlabel_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                                 label_y)\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_convergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/holbertonschool-machine_learning/mlenv/lib/python3.5/site-packages/GPyOpt/plotting/plots_bo.py\u001b[0m in \u001b[0;36mplot_acquisition\u001b[0;34m(bounds, input_dim, model, Xdata, Ydata, acquisition_function, suggested_sample, filename, label_x, label_y, color_by_step)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0macqu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquisition_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0macqu_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0macqu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0macqu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0macqu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0macqu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "my_bayesian_opt.plot_acquisition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 6.0,\n",
       " 7.0,\n",
       " 8.0,\n",
       " 9.0,\n",
       " 10.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 15.0,\n",
       " 16.0,\n",
       " 17.0,\n",
       " 18.0,\n",
       " 19.0,\n",
       " 20.0,\n",
       " 21.0,\n",
       " 22.0,\n",
       " 23.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 26.0,\n",
       " 27.0,\n",
       " 28.0,\n",
       " 29.0,\n",
       " 30.0)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "(np.linspace(1, 30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "np.arange(9).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Train on 51000 samples, validate on 9000 samples\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9792e66f2d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0macquisition_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"EI\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/holbertonschool-machine_learning/mlenv/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, domain, constraints, cost_withGradients, model_type, X, Y, initial_design_numdata, initial_design_type, acquisition_type, normalize_Y, exact_feval, acquisition_optimizer_type, model_update_interval, evaluator_type, batch_size, num_cores, verbosity, verbosity_model, maximize, de_duplication, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0minitial_design_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_design_chooser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# --- CHOOSE the model type. If an instance of a GPyOpt model is passed (possibly user defined), it is used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/holbertonschool-machine_learning/mlenv/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m_init_design_chooser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Case 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/holbertonschool-machine_learning/mlenv/lib/python3.5/site-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_procs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/holbertonschool-machine_learning/mlenv/lib/python3.5/site-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36m_eval_func\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mst_time\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mrlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mf_evals\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrlt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mcost_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9792e66f2d8f>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(hyperprameters)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     )\n\u001b[1;32m     93\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblackbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/holbertonschool-machine_learning/mlenv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/ml/holbertonschool-machine_learning/mlenv/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;31m# Reset stateful metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful_metric_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Hyperparameter Tuning\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import GPy\n",
    "import GPyOpt\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "\n",
    "# Importing dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Getting featurees and labels\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten data\n",
    "m, h, w = X_train.shape\n",
    "mv, _, _ = X_test.shape\n",
    "X_train = X_train.reshape(m, h * w)\n",
    "X_test = X_test.reshape(mv, h * w)\n",
    "\n",
    "# Data Normalization\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# Labels Encoding\n",
    "Y_train_oh = K.utils.to_categorical(Y_train)\n",
    "Y_test_oh = K.utils.to_categorical(Y_test)\n",
    "\n",
    "\n",
    "def create_model(lr, beta1, beta2, dropout_rate, l2_rate):\n",
    "    regularizer = K.regularizers.l2(l2_rate)\n",
    "\n",
    "    inputs = K.layers.Input(shape=(784,))\n",
    "    first = K.layers.Dense(\n",
    "        48,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer\n",
    "    )(inputs)\n",
    "    second = K.layers.Dense(\n",
    "        16,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer\n",
    "    )(first)\n",
    "    third = K.layers.Dense(\n",
    "        32,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer\n",
    "    )(second)\n",
    "    dropout = K.layers.Dropout(\n",
    "        dropout_rate\n",
    "    )(third)\n",
    "    outputs = K.layers.Dense(\n",
    "        10,\n",
    "        activation='softmax'\n",
    "    )(dropout)\n",
    "    model = K.Model(inputs, outputs)\n",
    "\n",
    "    adam = K.optimizers.Adam(\n",
    "        lr=lr,\n",
    "        beta_1=beta1,\n",
    "        beta_2=beta2\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=adam,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_model(hyperprameters):\n",
    "    \"\"\"Fit a model\"\"\"\n",
    "    lr = hyperprameters[0][0]\n",
    "    beta1 = hyperprameters[0][1]\n",
    "    beta2 = hyperprameters[0][2]\n",
    "    dropout_rate = hyperprameters[0][3]\n",
    "    l2_rate = hyperprameters[0][4]\n",
    "    batch_size = hyperprameters[0][5]\n",
    "    epochs = hyperprameters[0][6]\n",
    "\n",
    "    model = create_model(lr, beta1, beta2, dropout_rate, l2_rate)\n",
    "\n",
    "    blackbox = model.fit(\n",
    "        X_train,\n",
    "        Y_train_oh,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.15,\n",
    "    )\n",
    "    accuracy = blackbox.history['val_acc'][-1]\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "lr_domain = np.logspace(-9, 0, 10)\n",
    "beta1_domain = np.logspace(-9, 0, 10)\n",
    "beta2_domain = np.logspace(-9, 0, 10)\n",
    "dropout_domain = np.linspace(0, 0.9, 10)\n",
    "l2_domain = np.logspace(-9, 0, 10)\n",
    "batch_domain = np.linspace(10, 100, 10)\n",
    "epochs_domain = np.linspace(1, 30, 30)\n",
    "\n",
    "bounds = [\n",
    "    {\"name\": \"lr\", \"type\": \"continuous\", \"domain\": tuple(lr_domain)},\n",
    "    {\"name\": \"beta1\", \"type\": \"continuous\", \"domain\": tuple(beta1_domain)},\n",
    "    {\"name\": \"beta2\", \"type\": \"continuous\", \"domain\": tuple(beta2_domain)},\n",
    "    {\"name\": \"dropout_rate\", \"type\": \"continuous\", \"domain\": tuple(dropout_domain)},\n",
    "    {\"name\": \"l2_rate\", \"type\": \"continuous\", \"domain\": tuple(l2_domain)},\n",
    "    {\"name\": \"batch_size\", \"type\": \"continuous\", \"domain\": tuple(batch_domain)},\n",
    "    {\"name\": \"epochs\", \"type\": \"continuous\", \"domain\": tuple(epochs_domain)},\n",
    "]\n",
    "b_optimization = BayesianOptimization(\n",
    "    fit_model,\n",
    "    domain=bounds,\n",
    "    model_type=\"GP\",\n",
    "    initial_design_numdata=1,\n",
    "    acquisition_type=\"EI\",\n",
    "    maximize=False,\n",
    "    verbosity=True\n",
    ")\n",
    "\n",
    "b_optimization.run_optimization(max_iter=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "np.linspace(0, 0.9, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07,\n",
       "       1.e+08, 1.e+09])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "np.logspace(0, 9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "np.linspace(10, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr, beta1, beta2, dropout_rate, l2_rate):\n",
    "    regularizer = K.regularizers.l2(l2_rate)\n",
    "\n",
    "    inputs = K.layers.Input(shape=(784,))\n",
    "    first = K.layers.Dense(\n",
    "        48,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer\n",
    "    )(inputs)\n",
    "    second = K.layers.Dense(\n",
    "        16,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer\n",
    "    )(first)\n",
    "    third = K.layers.Dense(\n",
    "        32,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizer\n",
    "    )(second)\n",
    "    dropout = K.layers.Dropout(\n",
    "        dropout_rate\n",
    "    )(third)\n",
    "    outputs = K.layers.Dense(\n",
    "        10,\n",
    "        activation='softmax'\n",
    "    )(dropout)\n",
    "    model = K.Model(inputs, outputs)\n",
    "\n",
    "    adam = K.optimizers.Adam(\n",
    "        lr=lr,\n",
    "        beta_1=beta1,\n",
    "        beta_2=beta2\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=adam,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/10\n",
      "51000/51000 [==============================] - 3s 63us/step - loss: 3.9615 - acc: 0.1096 - val_loss: 2.4471 - val_acc: 0.1093\n",
      "Epoch 2/10\n",
      "51000/51000 [==============================] - 3s 53us/step - loss: 2.4244 - acc: 0.1049 - val_loss: 2.4257 - val_acc: 0.0959\n",
      "Epoch 3/10\n",
      "51000/51000 [==============================] - 3s 62us/step - loss: 2.4105 - acc: 0.1066 - val_loss: 2.4060 - val_acc: 0.1063\n",
      "Epoch 4/10\n",
      "51000/51000 [==============================] - 3s 57us/step - loss: 2.4079 - acc: 0.1066 - val_loss: 2.4086 - val_acc: 0.1063\n",
      "Epoch 5/10\n",
      "51000/51000 [==============================] - 3s 55us/step - loss: 2.4074 - acc: 0.1081 - val_loss: 2.4109 - val_acc: 0.1063\n",
      "Epoch 6/10\n",
      "51000/51000 [==============================] - 3s 58us/step - loss: 2.4082 - acc: 0.1021 - val_loss: 2.4190 - val_acc: 0.1093\n",
      "Epoch 7/10\n",
      "51000/51000 [==============================] - 3s 55us/step - loss: 2.4078 - acc: 0.1050 - val_loss: 2.4062 - val_acc: 0.1063\n",
      "Epoch 8/10\n",
      "51000/51000 [==============================] - 3s 55us/step - loss: 2.4078 - acc: 0.1043 - val_loss: 2.4125 - val_acc: 0.1063\n",
      "Epoch 9/10\n",
      "51000/51000 [==============================] - 3s 52us/step - loss: 2.4079 - acc: 0.1056 - val_loss: 2.4052 - val_acc: 0.1093\n",
      "Epoch 10/10\n",
      "51000/51000 [==============================] - 3s 54us/step - loss: 2.4082 - acc: 0.1057 - val_loss: 2.4115 - val_acc: 0.1028\n"
     ]
    }
   ],
   "source": [
    "model = create_model(.1, 0.9, .99, .2, .1)\n",
    "\n",
    "blackbox = model.fit(\n",
    "    X_train,\n",
    "    Y_train_oh,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_split=0.15,\n",
    ")\n",
    "accuracy = blackbox.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}